{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "data = pd.read_csv('../data/credit_risk_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique data</th>\n",
       "      <th>Missing data</th>\n",
       "      <th>Data types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>person_age</th>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person_income</th>\n",
       "      <td>4295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person_home_ownership</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person_emp_length</th>\n",
       "      <td>36</td>\n",
       "      <td>2.747000</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_intent</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_grade</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_int_rate</th>\n",
       "      <td>348</td>\n",
       "      <td>9.563856</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_status</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_percent_income</th>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Unique data  Missing data Data types\n",
       "person_age                           58      0.000000      int64\n",
       "person_income                      4295      0.000000      int64\n",
       "person_home_ownership                 4      0.000000     object\n",
       "person_emp_length                    36      2.747000    float64\n",
       "loan_intent                           6      0.000000     object\n",
       "loan_grade                            7      0.000000     object\n",
       "loan_amnt                           753      0.000000      int64\n",
       "loan_int_rate                       348      9.563856    float64\n",
       "loan_status                           2      0.000000      int64\n",
       "loan_percent_income                  77      0.000000    float64\n",
       "cb_person_default_on_file             2      0.000000     object\n",
       "cb_person_cred_hist_length           29      0.000000      int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the unique data for each column\n",
    "unique_data = pd.Series(data.nunique(), name=\"Unique data\")\n",
    "\n",
    "# Obtain the missing values of each column\n",
    "missing_data = pd.Series(data.isnull().mean() * 100, name=\"Missing data\")\n",
    "\n",
    "# Data types\n",
    "data_types = pd.Series(data.dtypes, name=\"Data types\")\n",
    "\n",
    "# Concatenate the results\n",
    "result = pd.concat([unique_data, missing_data, data_types], axis=1)\n",
    "\n",
    "# Print the results\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = data.drop(columns='loan_status')\n",
    "y = data['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19548, 11) (6516, 11) (6517, 11)\n",
      "(19548,) (6516,) (6517,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Show the shape of the train, validation and test sets\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to preprocess the numerical features\n",
    "\n",
    "# Select the numerical columns\n",
    "numerical_cols = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\n",
    "\n",
    "# Create the numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),          # Impute the missing values with the median\n",
    "    ('scaler', MinMaxScaler(feature_range=(0, 1))),         # Scale the data to be between 0 and 1\n",
    "    ('sqrt', FunctionTransformer(np.sqrt))                  # Apply the square root to the data\n",
    "])\n",
    "\n",
    "# Pipeline to preprocess the categorical or nominal features\n",
    "categorical_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n",
    "\n",
    "# Create the categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),   # Impute the missing values with the most frequent\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))      # One hot encode the data\n",
    "])\n",
    "\n",
    "# Combine the numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, numerical_cols),      # Apply the numerical pipeline to the numerical columns\n",
    "    ('categorical', categorical_pipeline, categorical_cols) # Apply the categorical pipeline to the categorical columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19548, 26) (6516, 26) (6517, 26)\n"
     ]
    }
   ],
   "source": [
    "# Fit the preprocessor\n",
    "X_train_preprocessed = preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train_preprocessed = preprocessor.transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Show the shape of the preprocessed data\n",
    "print(X_train_preprocessed.shape, X_val_preprocessed.shape, X_test_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_preprocessed, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_preprocessed, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Convert the target to PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19548, 26]) | torch.Size([19548])\n",
      "torch.Size([6516, 26]) | torch.Size([6516])\n",
      "torch.Size([6517, 26]) | torch.Size([6517])\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the train, validation and test sets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size)\n",
    "\n",
    "# Show the shape of the data\n",
    "print(X_train_tensor.shape, '|', y_train_tensor.shape)\n",
    "print(X_val_tensor.shape, '|' ,y_val_tensor.shape)\n",
    "print(X_test_tensor.shape, '|' ,y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class LoanApprovalNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LoanApprovalNN, self).__init__()              # Call the constructor of the parent class\n",
    "        self.fc1 = nn.Linear(X_train_tensor.shape[1], 64)   # Define the first fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 64)                        # Define the second fully connected layer\n",
    "        self.fc3 = nn.Linear(64, 64)                        # Define the third fully connected layer\n",
    "        self.fc4 = nn.Linear(64, 32)                        # Define the fourth fully connected layer\n",
    "        self.output = nn.Linear(32, 1)                      # Define the output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))             # Apply the ReLU activation function to the first fully connected layer\n",
    "        x = torch.relu(self.fc2(x))             # Apply the ReLU activation function to the second fully connected layer\n",
    "        x = torch.relu(self.fc3(x))             # Apply the ReLU activation function to the third fully connected layer\n",
    "        x = torch.relu(self.fc4(x))             # Apply the ReLU activation function to the fourth fully connected layer  \n",
    "        x = torch.sigmoid(self.output(x))       # Apply the sigmoid activation function to the output layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = LoanApprovalNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "criterion = nn.BCELoss()                                # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)    # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.3517, Validation Loss: 0.3157\n",
      "Epoch 2/50, Loss: 0.3032, Validation Loss: 0.2849\n",
      "Epoch 3/50, Loss: 0.2848, Validation Loss: 0.2715\n",
      "Epoch 4/50, Loss: 0.2717, Validation Loss: 0.2697\n",
      "Epoch 5/50, Loss: 0.2671, Validation Loss: 0.2627\n",
      "Epoch 6/50, Loss: 0.2571, Validation Loss: 0.2561\n",
      "Epoch 7/50, Loss: 0.2512, Validation Loss: 0.2484\n",
      "Epoch 8/50, Loss: 0.2460, Validation Loss: 0.2572\n",
      "Epoch 9/50, Loss: 0.2467, Validation Loss: 0.2458\n",
      "Epoch 10/50, Loss: 0.2455, Validation Loss: 0.2700\n",
      "Epoch 11/50, Loss: 0.2414, Validation Loss: 0.2416\n",
      "Epoch 12/50, Loss: 0.2385, Validation Loss: 0.2628\n",
      "Epoch 13/50, Loss: 0.2400, Validation Loss: 0.2805\n",
      "Epoch 14/50, Loss: 0.2354, Validation Loss: 0.2472\n",
      "Epoch 15/50, Loss: 0.2367, Validation Loss: 0.2466\n",
      "Epoch 16/50, Loss: 0.2351, Validation Loss: 0.2381\n",
      "Epoch 17/50, Loss: 0.2341, Validation Loss: 0.2379\n",
      "Epoch 18/50, Loss: 0.2324, Validation Loss: 0.2385\n",
      "Epoch 19/50, Loss: 0.2313, Validation Loss: 0.2434\n",
      "Epoch 20/50, Loss: 0.2300, Validation Loss: 0.2480\n",
      "Epoch 21/50, Loss: 0.2274, Validation Loss: 0.2311\n",
      "Epoch 22/50, Loss: 0.2288, Validation Loss: 0.2335\n",
      "Epoch 23/50, Loss: 0.2290, Validation Loss: 0.2449\n",
      "Epoch 24/50, Loss: 0.2262, Validation Loss: 0.2424\n",
      "Epoch 25/50, Loss: 0.2271, Validation Loss: 0.2318\n",
      "Epoch 26/50, Loss: 0.2235, Validation Loss: 0.2440\n",
      "Epoch 27/50, Loss: 0.2257, Validation Loss: 0.2322\n",
      "Epoch 28/50, Loss: 0.2248, Validation Loss: 0.2415\n",
      "Epoch 29/50, Loss: 0.2247, Validation Loss: 0.2381\n",
      "Epoch 30/50, Loss: 0.2214, Validation Loss: 0.2316\n",
      "Epoch 31/50, Loss: 0.2231, Validation Loss: 0.2380\n",
      "Epoch 32/50, Loss: 0.2213, Validation Loss: 0.2535\n",
      "Epoch 33/50, Loss: 0.2200, Validation Loss: 0.2299\n",
      "Epoch 34/50, Loss: 0.2188, Validation Loss: 0.2320\n",
      "Epoch 35/50, Loss: 0.2193, Validation Loss: 0.2351\n",
      "Epoch 36/50, Loss: 0.2193, Validation Loss: 0.2330\n",
      "Epoch 37/50, Loss: 0.2185, Validation Loss: 0.2409\n",
      "Epoch 38/50, Loss: 0.2196, Validation Loss: 0.2418\n",
      "Epoch 39/50, Loss: 0.2179, Validation Loss: 0.2519\n",
      "Epoch 40/50, Loss: 0.2205, Validation Loss: 0.2366\n",
      "Epoch 41/50, Loss: 0.2183, Validation Loss: 0.2349\n",
      "Epoch 42/50, Loss: 0.2166, Validation Loss: 0.2337\n",
      "Epoch 43/50, Loss: 0.2167, Validation Loss: 0.2280\n",
      "Epoch 44/50, Loss: 0.2152, Validation Loss: 0.2608\n",
      "Epoch 45/50, Loss: 0.2164, Validation Loss: 0.2519\n",
      "Epoch 46/50, Loss: 0.2153, Validation Loss: 0.2361\n",
      "Epoch 47/50, Loss: 0.2132, Validation Loss: 0.2457\n",
      "Epoch 48/50, Loss: 0.2131, Validation Loss: 0.2568\n",
      "Epoch 49/50, Loss: 0.2151, Validation Loss: 0.2631\n",
      "Epoch 50/50, Loss: 0.2127, Validation Loss: 0.2441\n"
     ]
    }
   ],
   "source": [
    "# Function to train the model\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                                       # Set the model to training mode\n",
    "        running_loss = 0.0                                                  # Initialize the running loss\n",
    "\n",
    "        for inputs, labels in train_loader:             \n",
    "            optimizer.zero_grad()                                           # Clear the gradients\n",
    "            outputs = model(inputs)                                         # Forward pass\n",
    "            loss = criterion(outputs.squeeze(), labels)                     # Compute the loss\n",
    "            loss.backward()                                                 # Backward pass\n",
    "            optimizer.step()                                                # Update the weights\n",
    "\n",
    "            running_loss += loss.item()                                     # Accumulate the loss\n",
    "\n",
    "        # Validation loss               \n",
    "        model.eval()                                                        # Set the model to evaluation mode\n",
    "        valid_loss = 0.0                                                    # Initialize the validation loss\n",
    "        with torch.no_grad():                                               # No gradients in validation\n",
    "            for inputs, labels in valid_loader:                             # Iterate over the validation loader\n",
    "                outputs = model(inputs)                                     # Forward pass\n",
    "                valid_loss += criterion(outputs.squeeze(), labels).item()   # Compute the loss\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {valid_loss/len(valid_loader):.4f}')\n",
    "\n",
    "# Train the model for 50 epochs and evaluate it on the validation set\n",
    "train(model, train_loader, val_loader, criterion, optimizer, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2536, Test Accuracy: 0.9179\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate(model, test_loader, criterion):                    # Function to evaluate the model\n",
    "    model.eval()                                                # Set the model to evaluation mode                 \n",
    "    test_loss = 0.0                                             # Initialize the test loss                       \n",
    "    correct = 0                                                 # Initialize the number of correct predictions\n",
    "    total = 0                                                   # Initialize the total number of predictions\n",
    "\n",
    "    with torch.no_grad():                                       # No gradients in evaluation\n",
    "        for inputs, labels in test_loader:                      # Iterate over the test loader\n",
    "            outputs = model(inputs).squeeze()                   # Forward pass\n",
    "            loss = criterion(outputs, labels)                   # Compute the loss\n",
    "            test_loss += loss.item()                            # Accumulate the loss\n",
    "            predicted = (outputs > 0.5).float()                 # Convert the probabilities to binary predictions\n",
    "            total += labels.size(0)                             # Accumulate the number of predictions\n",
    "            correct += (predicted == labels).sum().item()       # Accumulate the number of correct predictions\n",
    "\n",
    "    accuracy = correct / total                                  # Compute the accuracy\n",
    "\n",
    "    # Print the test loss and accuracy\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
